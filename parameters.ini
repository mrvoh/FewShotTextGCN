[Main]
experiment_name=transformers_test_jp
model_type=transformer
debug=false
patience=15
num_seeds=1

[TextPreprocessor]
num_similar_docs = 1
min_freq_word = 1
tokenizer_type = whitespace
language = japanese
preprocess_type = transformer
path_to_word2idx = word2idx.json
path_to_train_set = data/MLDoc/mldoc_japanese_train.tsv
path_to_test_set = data/MLDoc/mldoc_japanese_test.tsv
percentage_dev = 0.98

[LitTransformer]
transformer_model_name = bert-base-multilingual-cased

[LitTextGNN]
use_bbpe=false
num_subwords = 1000
lr = 5e-5
optimizer_type=ranger
use_most_similar_docs=false
subword_aggregator_type=none
use_self_training=false
self_training_conf_thresh=0.75
num_subwords = 1000

[LitSage]
tsa_schedule=log
use_unsup_loss=false
hidden_dim=64
dropout=0.5
num_layers=2
walks_per_node=2
walk_length=2
num_negative_samples=50
use_triplet_loss=false
triplet_margin=0.5
triplet_distance=cosine

[LitTransformer]
batch_size = 2


[Trainer]
log_every_n_steps=1
gpus=1
max_epochs=100
accumulate_grad_batches=10