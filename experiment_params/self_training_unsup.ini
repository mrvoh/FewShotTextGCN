[Main]
experiment_name=self_training
model_type=custom
debug=false
patience=1500
num_seeds=3

[TextPreprocessor]
use_bbpe=false
num_subwords = 1000
num_similar_docs = 5
min_freq_word = 5
tokenizer_type = whitespace
language = french
preprocess_type = textgcn
path_to_word2idx = word2idx.json
path_to_train_set = data/MLDoc/mldoc_french_train.tsv
path_to_test_set = data/MLDoc/mldoc_french_test.tsv
percentage_dev = 0.9

[LitTextGNN]
lr = 0.01
optimizer_type=ranger
use_most_similar_docs=false
tsa_schedule=exp
subword_aggregator_type=none
use_self_training=true
self_training_conf_thresh=0.75

[LitSage]
tsa_schedule=log
use_unsup_loss=true
hidden_dim=64
dropout=0.5
num_layers=2
walks_per_node=2
walk_length=2
num_negative_samples=50
use_triplet_loss=true
triplet_margin=0.5
triplet_distance=cosine

[GraphSageTextDataset]
batch_size = 16
num_neighbors_hop1 = 500
num_neighbors_hop2 = 20

[Trainer]
log_every_n_steps=1
gpus=1
max_epochs=1000