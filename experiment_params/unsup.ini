[Main]
experiment_name=baserun_custom_jp_with_callback
debug=false
patience=500
model_type=custom
num_seeds=5

[TextPreprocessor]
use_bbpe=false
num_subwords = 2500
num_similar_docs = 5
min_freq_word = 5
tokenizer_type = japanese
language = japanese
preprocess_type = textgcn
path_to_word2idx = word2idx.json
path_to_train_set = data/MLDoc/mldoc_japanese_train.tsv
path_to_test_set = data/MLDoc/mldoc_japanese_test.tsv
percentage_dev = 0.98

[LitTextGNN]
lr = 0.01
optimizer_type=ranger
use_most_similar_docs=false
tsa_schedule=log
subword_aggregator_type=transformer

[LitSage]
use_unsup_loss=true
hidden_dim=100
dropout=0.0
num_layers=2
walks_per_node=2
walk_length=2
num_negative_samples=50
use_triplet_loss=true
triplet_margin=0.5
triplet_distance=cosine

[GraphSageTextDataset]
batch_size = 16
num_neighbors_hop1 = 500
num_neighbors_hop2 = 20

[Trainer]
log_every_n_steps=1
gpus=1
max_epochs=1000
enable_checkpointing=true