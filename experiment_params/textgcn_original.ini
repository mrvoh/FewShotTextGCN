[Main]
experiment_name=Ohsumed_textGCN_baseline_adam
model_type=gcn
debug=false
patience=500
num_seeds=5

[TextPreprocessor]
use_bbpe=false
num_subwords = 1000
num_similar_docs = 5
min_freq_word = 5
tokenizer_type = whitespace
language = english
preprocess_type = textgcn
path_to_word2idx = word2idx.json
path_to_train_set = data/ohsumed_train.tsv
path_to_test_set = data/ohsumed_test.tsv
percentage_dev = 0.1

[LitTextGNN]
lr = 0.02
optimizer_type=adam
use_most_similar_docs=false
subword_aggregator_type=none
use_self_training=false
self_training_conf_thresh=0.75

[LitSage]
tsa_schedule=log
use_unsup_loss=false
hidden_dim=200
dropout=0.5
num_layers=2
walks_per_node=2
walk_length=2
num_negative_samples=50
use_triplet_loss=true
triplet_margin=0.5
triplet_distance=cosine

[GraphSageTextDataset]
batch_size = 16
num_neighbors_hop1 = 500
num_neighbors_hop2 = 20

[Trainer]
log_every_n_steps=1
gpus=1
max_epochs=1000